#!/usr/bin/env python3
"""
Script para comparar modelos HOG y BRISK.
"""

import json
import matplotlib.pyplot as plt
import numpy as np

print("\n" + "="*70)
print(" COMPARACI√ìN DE MODELOS HOG vs BRISK")
print("="*70 + "\n")

# Cargar m√©tricas
with open('results/detection_hog_metrics.json', 'r') as f:
    hog_metrics = json.load(f)

with open('results/detection_brisk_metrics.json', 'r') as f:
    brisk_metrics = json.load(f)

# Imprimir comparaci√≥n
print("üìä M√âTRICAS DE EVALUACI√ìN")
print("-" * 70)
print(f"{'M√©trica':<30} {'HOG':>15} {'BRISK':>15} {'Mejor':>10}")
print("-" * 70)

metrics = [
    ('IoU Promedio', 'avg_iou'),
    ('IoU Mediana', 'median_iou'),
    ('Desv. Est. IoU', 'std_iou'),
    ('IoU M√≠nimo', 'min_iou'),
    ('IoU M√°ximo', 'max_iou'),
    ('MAE', 'mae'),
    ('Precisi√≥n @ IoU‚â•0.5', 'accuracy@0.5'),
    ('Precisi√≥n @ IoU‚â•0.75', 'accuracy@0.75'),
    ('Precisi√≥n @ IoU‚â•0.9', 'accuracy@0.9'),
]

for name, key in metrics:
    hog_val = hog_metrics[key]
    brisk_val = brisk_metrics[key]
    
    # Determinar cu√°l es mejor (mayor es mejor excepto para MAE y std)
    if key in ['mae', 'std_iou', 'min_iou']:
        mejor = 'HOG' if hog_val <= brisk_val else 'BRISK'
    else:
        mejor = 'HOG' if hog_val >= brisk_val else 'BRISK'
    
    if key.startswith('accuracy'):
        # Mostrar como porcentaje
        print(f"{name:<30} {hog_val*100:>14.2f}% {brisk_val*100:>14.2f}% {mejor:>10}")
    else:
        print(f"{name:<30} {hog_val:>15.4f} {brisk_val:>15.4f} {mejor:>10}")

print("-" * 70)

# Conteo de victorias
hog_wins = sum([
    hog_metrics['avg_iou'] > brisk_metrics['avg_iou'],
    hog_metrics['median_iou'] > brisk_metrics['median_iou'],
    hog_metrics['max_iou'] > brisk_metrics['max_iou'],
    hog_metrics['mae'] < brisk_metrics['mae'],
    hog_metrics['accuracy@0.5'] > brisk_metrics['accuracy@0.5'],
    hog_metrics['accuracy@0.75'] > brisk_metrics['accuracy@0.75'],
    hog_metrics['accuracy@0.9'] > brisk_metrics['accuracy@0.9'],
])

brisk_wins = 7 - hog_wins

print(f"\nüèÜ RESUMEN:")
print(f"   HOG gana en:   {hog_wins}/7 m√©tricas principales")
print(f"   BRISK gana en: {brisk_wins}/7 m√©tricas principales")

if hog_wins > brisk_wins:
    print(f"\n‚úÖ GANADOR: HOG (mejor rendimiento general)")
else:
    print(f"\n‚úÖ GANADOR: BRISK (mejor rendimiento general)")

# Informaci√≥n del modelo
print("\n" + "="*70)
print("‚ÑπÔ∏è  INFORMACI√ìN DE MODELOS")
print("-" * 70)
print(f"{'Caracter√≠stica':<30} {'HOG':>20} {'BRISK':>20}")
print("-" * 70)
print(f"{'Dimensi√≥n de features':<30} {42849:>20} {32768:>20}")
print(f"{'Total de muestras':<30} {hog_metrics['total_samples']:>20} {brisk_metrics['total_samples']:>20}")
print(f"{'Correctos @ IoU‚â•0.5':<30} {hog_metrics['correct@0.5']:>20} {brisk_metrics['correct@0.5']:>20}")
print(f"{'Correctos @ IoU‚â•0.75':<30} {hog_metrics['correct@0.75']:>20} {brisk_metrics['correct@0.75']:>20}")
print(f"{'Correctos @ IoU‚â•0.9':<30} {hog_metrics['correct@0.9']:>20} {brisk_metrics['correct@0.9']:>20}")
print("-" * 70)

print("\n" + "="*70)
print("‚úÖ Comparaci√≥n completada")
print("="*70 + "\n")

# Crear gr√°fico de comparaci√≥n
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Comparaci√≥n de IoU
ax = axes[0, 0]
metrics_names = ['Promedio', 'Mediana', 'M√°ximo']
hog_values = [hog_metrics['avg_iou'], hog_metrics['median_iou'], hog_metrics['max_iou']]
brisk_values = [brisk_metrics['avg_iou'], brisk_metrics['median_iou'], brisk_metrics['max_iou']]

x = np.arange(len(metrics_names))
width = 0.35

ax.bar(x - width/2, hog_values, width, label='HOG', color='skyblue')
ax.bar(x + width/2, brisk_values, width, label='BRISK', color='lightcoral')
ax.set_ylabel('IoU', fontsize=12)
ax.set_title('Comparaci√≥n de IoU', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# 2. Comparaci√≥n de Precisi√≥n
ax = axes[0, 1]
thresholds = ['IoU‚â•0.5', 'IoU‚â•0.75', 'IoU‚â•0.9']
hog_acc = [hog_metrics['accuracy@0.5']*100, hog_metrics['accuracy@0.75']*100, hog_metrics['accuracy@0.9']*100]
brisk_acc = [brisk_metrics['accuracy@0.5']*100, brisk_metrics['accuracy@0.75']*100, brisk_metrics['accuracy@0.9']*100]

x = np.arange(len(thresholds))
ax.bar(x - width/2, hog_acc, width, label='HOG', color='skyblue')
ax.bar(x + width/2, brisk_acc, width, label='BRISK', color='lightcoral')
ax.set_ylabel('Precisi√≥n (%)', fontsize=12)
ax.set_title('Precisi√≥n en diferentes umbrales de IoU', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(thresholds)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

# 3. MAE Comparison
ax = axes[1, 0]
models = ['HOG', 'BRISK']
mae_values = [hog_metrics['mae'], brisk_metrics['mae']]
colors = ['skyblue', 'lightcoral']

ax.bar(models, mae_values, color=colors, alpha=0.7)
ax.set_ylabel('MAE', fontsize=12)
ax.set_title('Error Absoluto Medio (MAE)', fontsize=14, fontweight='bold')
ax.grid(True, alpha=0.3, axis='y')

for i, v in enumerate(mae_values):
    ax.text(i, v + 0.002, f'{v:.4f}', ha='center', fontweight='bold')

# 4. Distribuci√≥n de predicciones correctas
ax = axes[1, 1]
categories = ['‚â•0.5', '‚â•0.75', '‚â•0.9']
hog_correct = [hog_metrics['correct@0.5'], hog_metrics['correct@0.75'], hog_metrics['correct@0.9']]
brisk_correct = [brisk_metrics['correct@0.5'], brisk_metrics['correct@0.75'], brisk_metrics['correct@0.9']]

x = np.arange(len(categories))
ax.bar(x - width/2, hog_correct, width, label='HOG', color='skyblue')
ax.bar(x + width/2, brisk_correct, width, label='BRISK', color='lightcoral')
ax.set_ylabel('N√∫mero de predicciones correctas', fontsize=12)
ax.set_title('Predicciones correctas por umbral de IoU', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(categories)
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('results/hog_vs_brisk_comparison.png', dpi=150, bbox_inches='tight')
print("üìä Gr√°fico de comparaci√≥n guardado: results/hog_vs_brisk_comparison.png")
